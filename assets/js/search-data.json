{
  
    
        "post0": {
            "title": "Print Simple Tables",
            "content": "Manually Created Tables . Basic Table w/ Manual Column Size Control . The sample data is Covid-19 region 8 information pulled from the Illinois Department of Public Health. More data than is needed comes with this HTTP request. | Specific data used is for the County Test Positivity Reports. | . | Code description: line 8: printout of sample json object to show what the raw data looks like | line 11: Simplest form of creating a header: string printing of strings | line 14: The json object, &#39;countyTestPositivities&#39;, under &#39;CountryTestPositivityReports&#39; contains a list of json objects inside - one for each county. The nested for loop takes the first object, loops through the nested list, then moves onto the next object and nested list. | lines 16-26: rudementry data clean up for the plain text table This clean up involves both reformatting of data, but also ensuring that the cells in the table all line up. | Example - Data Formatting: &quot;date&quot; - counties[&#39;reportDate&#39;][:10] takes the long datetime string and moves all of the time characters. It does this by cutting out any characters between the 10th character ([9]) in the string. There are better ways to do this using the datetime library, but this works as well. | Example - Table Formatting: &quot;name&quot; We want the name column to always be 1 characters wide. | The first step is the basic action of setting the name variable from the pulled data | The second step is the following, in reverse order of the code in line 19: (1) count (len) the number of characters in the name string | (2) subtract that number from the desired column width, which is 10 in this case (if the name is &quot;Kane&quot;, then that would be 10 - 4 = 6) | (3) populate &quot;name_fill&quot; with a string of spaces the length of the result from the previous step (if the name is &quot;Kane&quot;, then that would be a string of 6 spaces) | . | . | The column filler variable is setup for &quot;name&quot;, &quot;positive&quot;, &quot;total&quot;, &quot;daily_avg&quot;, and &quot;seven_day_avg&quot; | . | line 27: The content portion of the table is printed one line at a time using a combination of bars, spaces, and the pair of data variable and filler variable | line 28: Closes the bottm of the table with another string to match the header | . | See the output further below for a visual of the raw data example, and the resulting plain text table | . print(build_title(&#39;COUNTY TEST POSITIVITY REPORTS&#39;)) query_url = &#39;https://idph.illinois.gov/DPHPublicInformation/api/COVID/GetResurgenceData?regionID=8&amp;daysIncluded=5&#39; #+ selectedRegion + &#39;&amp;daysIncluded=&#39; + chartRange page = requests.get(query_url) positivity_rates = page.json()[&#39;CountyTestPositivityReports&#39;] print(&#39;SINGLE SAMPLE&#39;) pprint.pprint(positivity_rates[0]) print() print(&#39;+--+&#39;) print(&#39;| DATE | NAME | POS | TOTAL | AVG | 7 DAY |&#39;) print(&#39;|--|&#39;) for counties in positivity_rates: for i in counties[&#39;countyTestPositivities&#39;]: date = counties[&#39;reportDate&#39;][:10] name = i[&#39;CountyName&#39;] name_fill = &#39; &#39;*(10 - len(name)) positive = i[&#39;positive_test&#39;] positive_fill = &#39; &#39;*(6 - len(str(positive))) total = i[&#39;totalTest&#39;] total_fill = &#39; &#39;*(6 - len(str(total))) daily_avg = round(float(i[&#39;positive_test&#39;]/i[&#39;totalTest&#39;]*100), 1) # positive tests / total tests * 100, then rounded to 1 decimal place daily_avg_fill = &#39; &#39; *(5 - len(str(daily_avg))) seven_day_avg = i[&#39;positivityRollingAvg&#39;] seven_fill = &#39; &#39;*(5 - len(str(seven_day_avg))) print(f&#39;| {date} | {name}{name_fill} | {positive}{positive_fill} | {total}{total_fill} | {daily_avg}{daily_avg_fill} | {seven_day_avg}{seven_fill} |&#39;) print(&#39;+--+&#39;) . - COUNTY TEST POSITIVITY REPORTS - SINGLE SAMPLE {&#39;countyTestPositivities&#39;: [{&#39;CountyName&#39;: &#39;DuPage&#39;, &#39;dailyPositivity&#39;: 0.0, &#39;positive_test&#39;: 747, &#39;positivityRollingAvg&#39;: 13.5, &#39;regionID&#39;: 8, &#39;totalTest&#39;: 5344}, {&#39;CountyName&#39;: &#39;Kane&#39;, &#39;dailyPositivity&#39;: 0.0, &#39;positive_test&#39;: 550, &#39;positivityRollingAvg&#39;: 16.4, &#39;regionID&#39;: 8, &#39;totalTest&#39;: 3792}], &#39;reportDate&#39;: &#39;2020-11-19T00:00:00&#39;} +--+ | DATE | NAME | POS | TOTAL | AVG | 7 DAY | |--| | 2020-11-19 | DuPage | 747 | 5344 | 14.0 | 13.5 | | 2020-11-19 | Kane | 550 | 3792 | 14.5 | 16.4 | | 2020-11-20 | DuPage | 739 | 5755 | 12.8 | 13.2 | | 2020-11-20 | Kane | 706 | 3646 | 19.4 | 16.7 | | 2020-11-21 | DuPage | 629 | 4822 | 13.0 | 13.1 | | 2020-11-21 | Kane | 486 | 2367 | 20.5 | 17.0 | | 2020-11-22 | DuPage | 789 | 6494 | 12.1 | 13.0 | | 2020-11-22 | Kane | 477 | 3126 | 15.3 | 16.8 | | 2020-11-23 | DuPage | 613 | 6652 | 9.2 | 12.3 | | 2020-11-23 | Kane | 323 | 2838 | 11.4 | 16.4 | | 2020-11-24 | DuPage | 777 | 6411 | 12.1 | 12.3 | | 2020-11-24 | Kane | 517 | 3790 | 13.6 | 16.3 | +--+ . Basic Table with Cleaned Up Data (and also w/ Manual Column Size Control) . import datetime from datetime import date # TABLE PARAMETERS line_length = 59 print(build_title(&#39;COUNTY TEST POSITIVITY REPORTS&#39;)) query_url = &#39;https://idph.illinois.gov/DPHPublicInformation/api/COVID/GetResurgenceData?regionID=8&amp;daysIncluded=10&#39; #+ selectedRegion + &#39;&amp;daysIncluded=&#39; + chartRange page = requests.get(query_url) positivity_rates = page.json()[&#39;CountyTestPositivityReports&#39;] print(&#39;SINGLE SAMPLE&#39;) pprint.pprint(positivity_rates[-1]) behind = datetime.datetime.strptime(positivity_rates[-1][&#39;reportDate&#39;], &quot;%Y-%m-%dT%H:%M:%S&quot;) behind_str = f&#39;CURRENT DATE: {date.today()} (Report is {(date.today() - behind.date()).days} days *behind*)&#39; behind_fill = &#39; &#39;*(57 - len(behind_str)) print() print(f&#39;+{&quot;=&quot;*line_length}+&#39;) print(f&#39;| {behind_str}{behind_fill} |&#39;) print(f&#39;|{&quot;=&quot;*line_length}|&#39;) print(&#39;| NAME | DATE | POS | TOTAL | AVG | 7 DAY |&#39;) print(f&#39;|{&quot;-&quot;*line_length}|&#39;) number_of_counties = len(positivity_rates[0][&#39;countyTestPositivities&#39;]) for n in range(number_of_counties): print_name = True for counties in positivity_rates: county = counties[&#39;countyTestPositivities&#39;][n]#[n-1] date = counties[&#39;reportDate&#39;][:10] name = county[&#39;CountyName&#39;] if print_name == True else &#39;&#39; name_fill = &#39; &#39;*(10 - len(name)) positive = county[&#39;positive_test&#39;] positive_fill = &#39; &#39;*(6 - len(str(positive))) total = county[&#39;totalTest&#39;] total_fill = &#39; &#39;*(6 - len(str(total))) daily_avg = round(float(county[&#39;positive_test&#39;]/county[&#39;totalTest&#39;]*100), 1) # positive tests / total tests * 100, then rounded to 1 decimal place daily_avg_fill = &#39; &#39; *(5 - len(str(daily_avg))) seven_day_avg = county[&#39;positivityRollingAvg&#39;] seven_fill = &#39; &#39;*(5 - len(str(seven_day_avg))) print(f&#39;| {name}{name_fill} | {date} | {positive}{positive_fill} | {total}{total_fill} | {daily_avg}{daily_avg_fill} | {seven_day_avg}{seven_fill} |&#39;) print_name = False print(f&#39;+{&quot;-&quot;*line_length}+&#39;) . - COUNTY TEST POSITIVITY REPORTS - SINGLE SAMPLE {&#39;countyTestPositivities&#39;: [{&#39;CountyName&#39;: &#39;DuPage&#39;, &#39;dailyPositivity&#39;: 0.0, &#39;positive_test&#39;: 777, &#39;positivityRollingAvg&#39;: 12.3, &#39;regionID&#39;: 8, &#39;totalTest&#39;: 6411}, {&#39;CountyName&#39;: &#39;Kane&#39;, &#39;dailyPositivity&#39;: 0.0, &#39;positive_test&#39;: 517, &#39;positivityRollingAvg&#39;: 16.3, &#39;regionID&#39;: 8, &#39;totalTest&#39;: 3790}], &#39;reportDate&#39;: &#39;2020-11-24T00:00:00&#39;} +===========================================================+ | CURRENT DATE: 2020-11-28 (Report is 4 days *behind*) | |===========================================================| | NAME | DATE | POS | TOTAL | AVG | 7 DAY | |--| | DuPage | 2020-11-14 | 695 | 4973 | 14.0 | 14.6 | | | 2020-11-15 | 699 | 5628 | 12.4 | 14.1 | | | 2020-11-16 | 756 | 5619 | 13.5 | 13.9 | | | 2020-11-17 | 695 | 5626 | 12.4 | 13.5 | | | 2020-11-18 | 817 | 6029 | 13.6 | 13.5 | | | 2020-11-19 | 747 | 5344 | 14.0 | 13.5 | | | 2020-11-20 | 739 | 5755 | 12.8 | 13.2 | | | 2020-11-21 | 629 | 4822 | 13.0 | 13.1 | | | 2020-11-22 | 789 | 6494 | 12.1 | 13.0 | | | 2020-11-23 | 613 | 6652 | 9.2 | 12.3 | | | 2020-11-24 | 777 | 6411 | 12.1 | 12.3 | +--+ | Kane | 2020-11-14 | 458 | 2570 | 17.8 | 18.0 | | | 2020-11-15 | 457 | 2673 | 17.1 | 17.5 | | | 2020-11-16 | 430 | 3034 | 14.2 | 17.2 | | | 2020-11-17 | 458 | 3236 | 14.2 | 16.7 | | | 2020-11-18 | 639 | 3143 | 20.3 | 17.1 | | | 2020-11-19 | 550 | 3792 | 14.5 | 16.4 | | | 2020-11-20 | 706 | 3646 | 19.4 | 16.7 | | | 2020-11-21 | 486 | 2367 | 20.5 | 17.0 | | | 2020-11-22 | 477 | 3126 | 15.3 | 16.8 | | | 2020-11-23 | 323 | 2838 | 11.4 | 16.4 | | | 2020-11-24 | 517 | 3790 | 13.6 | 16.3 | +--+ . Tables Created with tabulate . Regular tabulate table . Pandas tabulate table . tablefmt options: plain, simple, github, grid, fancy_grid, pipe, orgtbl, jira, fpresto, pretty, psql, rst, mediawiki, moinmoin, youtrack, html, latex, latex_raw, latex_booktabs, textile | . import numpy as np import pandas as pd from tabulate import tabulate import requests import datetime query_url = &#39;https://idph.illinois.gov/DPHPublicInformation/api/COVID/GetResurgenceData?regionID=8&amp;daysIncluded=10&#39; # max is 170 days #+ selectedRegion + &#39;&amp;daysIncluded=&#39; + chartRange page = requests.get(query_url) positivity_rates = page.json()[&#39;CountyTestPositivityReports&#39;] flattened_positivity_rates = [] for dates in positivity_rates: for county in dates[&#39;countyTestPositivities&#39;]: county[&#39;date&#39;] = datetime.datetime.strptime(dates[&#39;reportDate&#39;], &quot;%Y-%m-%dT%H:%M:%S&quot;) flattened_positivity_rates.append(county) county_test_pos_rates_df = pd.DataFrame(flattened_positivity_rates) county_test_pos_rates_df[&#39;daily_avg&#39;] = (county_test_pos_rates_df[&#39;positive_test&#39;] / county_test_pos_rates_df[&#39;totalTest&#39;]) * 100 print(tabulate(county_test_pos_rates_df.sort_values([&#39;CountyName&#39;, &#39;date&#39;]), headers = &#39;keys&#39;, tablefmt = &#39;psql&#39;)) . +-+--+-+--++-+++-+ | | CountyName | totalTest | positive_test | positivityRollingAvg | dailyPositivity | regionID | date | daily_avg | |-+--+-+--++-+++-| | 0 | DuPage | 4973 | 695 | 14.6 | 0 | 8 | 2020-11-14 00:00:00 | 13.9755 | | 2 | DuPage | 5628 | 699 | 14.1 | 0 | 8 | 2020-11-15 00:00:00 | 12.42 | | 4 | DuPage | 5619 | 756 | 13.9 | 0 | 8 | 2020-11-16 00:00:00 | 13.4544 | | 6 | DuPage | 5626 | 695 | 13.5 | 0 | 8 | 2020-11-17 00:00:00 | 12.3534 | | 8 | DuPage | 6029 | 817 | 13.5 | 0 | 8 | 2020-11-18 00:00:00 | 13.5512 | | 10 | DuPage | 5344 | 747 | 13.5 | 0 | 8 | 2020-11-19 00:00:00 | 13.9783 | | 12 | DuPage | 5755 | 739 | 13.2 | 0 | 8 | 2020-11-20 00:00:00 | 12.841 | | 14 | DuPage | 4822 | 629 | 13.1 | 0 | 8 | 2020-11-21 00:00:00 | 13.0444 | | 16 | DuPage | 6494 | 789 | 13 | 0 | 8 | 2020-11-22 00:00:00 | 12.1497 | | 18 | DuPage | 6652 | 613 | 12.3 | 0 | 8 | 2020-11-23 00:00:00 | 9.21527 | | 20 | DuPage | 6411 | 777 | 12.3 | 0 | 8 | 2020-11-24 00:00:00 | 12.1198 | | 1 | Kane | 2570 | 458 | 18 | 0 | 8 | 2020-11-14 00:00:00 | 17.821 | | 3 | Kane | 2673 | 457 | 17.5 | 0 | 8 | 2020-11-15 00:00:00 | 17.0969 | | 5 | Kane | 3034 | 430 | 17.2 | 0 | 8 | 2020-11-16 00:00:00 | 14.1727 | | 7 | Kane | 3236 | 458 | 16.7 | 0 | 8 | 2020-11-17 00:00:00 | 14.1533 | | 9 | Kane | 3143 | 639 | 17.1 | 0 | 8 | 2020-11-18 00:00:00 | 20.3309 | | 11 | Kane | 3792 | 550 | 16.4 | 0 | 8 | 2020-11-19 00:00:00 | 14.5042 | | 13 | Kane | 3646 | 706 | 16.7 | 0 | 8 | 2020-11-20 00:00:00 | 19.3637 | | 15 | Kane | 2367 | 486 | 17 | 0 | 8 | 2020-11-21 00:00:00 | 20.5323 | | 17 | Kane | 3126 | 477 | 16.8 | 0 | 8 | 2020-11-22 00:00:00 | 15.2591 | | 19 | Kane | 2838 | 323 | 16.4 | 0 | 8 | 2020-11-23 00:00:00 | 11.3813 | | 21 | Kane | 3790 | 517 | 16.3 | 0 | 8 | 2020-11-24 00:00:00 | 13.6412 | +-+--+-+--++-+++-+ . Aside: Same Pandas Data Visualized . import matplotlib.pyplot as plt import matplotlib as mpl from datetime import datetime import numpy as np import pandas as pd import requests query_url = &#39;https://idph.illinois.gov/DPHPublicInformation/api/COVID/GetResurgenceData?regionID=8&amp;daysIncluded=90&#39; # max is 170 days #+ selectedRegion + &#39;&amp;daysIncluded=&#39; + chartRange page = requests.get(query_url) positivity_rates = page.json()[&#39;CountyTestPositivityReports&#39;] flattened_positivity_rates = [] for dates in positivity_rates: for county in dates[&#39;countyTestPositivities&#39;]: county[&#39;date&#39;] = datetime.strptime(dates[&#39;reportDate&#39;], &quot;%Y-%m-%dT%H:%M:%S&quot;) flattened_positivity_rates.append(county) county_test_pos_rates_df = pd.DataFrame(flattened_positivity_rates) county_test_pos_rates_df[&#39;daily_avg&#39;] = (county_test_pos_rates_df[&#39;positive_test&#39;] / county_test_pos_rates_df[&#39;totalTest&#39;]) * 100 mpl.rcParams[&#39;figure.dpi&#39;] = 150 county_test_pos_rates_df.plot(y=[&#39;daily_avg&#39;, &#39;positivityRollingAvg&#39;], x=&#39;date&#39;); . # SAME GRAPH, BUT WITH INTERACTIVE ADJUSTMENTS import matplotlib.pyplot as plt import matplotlib as mpl from ipywidgets import interact from datetime import datetime, timedelta import numpy as np import pandas as pd import requests query_url = &#39;https://idph.illinois.gov/DPHPublicInformation/api/COVID/GetResurgenceData?regionID=8&amp;daysIncluded=90&#39; # max is 170 days #+ selectedRegion + &#39;&amp;daysIncluded=&#39; + chartRange page = requests.get(query_url) positivity_rates = page.json()[&#39;CountyTestPositivityReports&#39;] flattened_positivity_rates = [] for dates in positivity_rates: for county in dates[&#39;countyTestPositivities&#39;]: county[&#39;date&#39;] = datetime.strptime(dates[&#39;reportDate&#39;], &quot;%Y-%m-%dT%H:%M:%S&quot;) flattened_positivity_rates.append(county) county_test_pos_rates_df = pd.DataFrame(flattened_positivity_rates) county_test_pos_rates_df[&#39;daily_avg&#39;] = (county_test_pos_rates_df[&#39;positive_test&#39;] / county_test_pos_rates_df[&#39;totalTest&#39;]) * 100 @interact(dpi=(100, 500), days=(30, 93, 7)) def graph(dpi=100, days=93): mpl.rcParams[&#39;figure.dpi&#39;] = dpi number_prev_days = datetime.today() - timedelta (days) filtered_df=county_test_pos_rates_df.query(f&quot;date &lt;= &#39;{datetime.today()}&#39; and date &gt;= &#39;{number_prev_days}&#39;&quot;) filtered_df.plot(y=[&#39;daily_avg&#39;, &#39;positivityRollingAvg&#39;], x=&#39;date&#39;) . .",
            "url": "https://darrida.com/general/2020/11/23/Print-Simple-Tables.html",
            "relUrl": "/general/2020/11/23/Print-Simple-Tables.html",
            "date": " • Nov 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Tranforming and Flattening Data",
            "content": "The Situation . A while back I had a the need to take what were essentially audit logs, where multiple records existed for each id, and flatten then into a single record for each id. . I pulled the data into python from a database as a list of over 100,000 tuples. Here is an example of the data I was looking at. . csv (audit_record.csv) RECORD,ID,OLD_VALUE,FINAL_VALUE,DATE,TABLE,COLUMN 1, id1, value1, value2, 02/01/2020, table1, column1 2, id1, value2, value3, 02/02/2020, table1, column1 3, id1, value3, value1, 02/03/2020, table1, column1 4, id2, value4, value5, 02/01/2020, table1, column1 5, id2, value5, , 02/03/2020, table1, column1 6, id3, value6, value7, 02/01/2020, table1, column1 7, id3, value8, value9, 02/02/2020, table1, column1 8, id4, value10, value11, 02/01/2020, table1, column1 9, id4, value11, value12, 02/02/2020, table1, column1 10,id4, value12, value4, 02/03/2020, table1, column1 . The Problem . This was an audit log for a series of undesired changes. Here is an example of the progression of these changes: . 02/01/2020: a large number of records were changed and iterated to the next highest available number | 02/02/2020: a similar number of records (but not all) were changed again, again iterating to the next available number | 02/03/2020: an additional event took place that resulted in many of the records being corrected (returning to their previous numbers), but some ended at yet another iteration higher, while still others ended up missing a value altogether. | . In the sample information above there are 4 different unintended transformations of data that occurred: . id1: Example series of changes that ended back at the correct value: . 02/01: value1 =&gt; value2 | 02/02: value2 =&gt; value3 | 02/03: value3 =&gt; value1 . id2: Example of series of changes that ended with a complete removal of the value: . 02/01: value4 =&gt; value5 | 02/02: No Changes | 02/03: value5 =&gt; null . id3: Example of series of changes that ended at a different number: . 02/01: value6 =&gt; value7 | 02/02: value7 =&gt; value8 | 02/03: No Changes . id4: Example of series of changes that ended up a different id&#39;s value (data not only incorrect, but conflicting): . 02/01: value10 =&gt; value11 | 02/02: value11 =&gt; value12 | 02/03: value12 =&gt; value4 . Details . In addition to the obvious differences between these 4 different changes, there are some that are harder to see: . Some id records went through 3 changes, others went through 2 (this one is not hard to see) | The last change of some id records was on 02/02, while others&#39; last change was on 02/03 | . Altogether, the different factors that need to be understood are the following: . Some records changed each of the 3 days, some only on 2 | Some of the records that only changed 2 times had their final changed on day 2, while others skipped day 2 and had their final change on day 3 | Some records eventually were self corrected to their original value | Some records iterated 2 times and ended at a different number | Some records eventually ended up with a final value of null | Some records ended up at the original value of a different id | . This example, as messy as it is, also is cleaner that the situation itself. The follow challenges existed: . There were some that only had 1 audit record where the original value was immediately replaced with a null | The list of tuples was not sorted in any fashion | The dates were actually spread over a period of 14 or 15 days, with the changes for a single record following anywhere in that time period -- not a clean 3 days like the example here. | . The tuples that represent the audits for these changes are all over the place. In the end, what I wanted to see clearly what I was dealing with. In order to do that I needed to have a list of new records that would clearly show me the following for each id: . original value, and the date that value was lost | final value, and the date that value was added | . The Solution . Gather the change associated with a single id together, identified by that id: | . import csv from collections import defaultdict reader = csv.DictReader(open(&#39;2020-06-08/audit_record.csv&#39;)) dict_by_user = defaultdict(list) for i in reader: dict_by_user[i[&#39;ID&#39;]].append(i) . The results in dcit_by_user are the following data structure (python dictionary) | . dict_by_user = { &#39;id1&#39;: [[&#39;id1&#39;, &#39;value1&#39;, &#39;value2&#39;, &#39;02/01/2020&#39;, &#39;table1&#39;, &#39;column1&#39;], [&#39;id1&#39;, &#39;value2&#39;, &#39;value3&#39;, &#39;02/02/2020&#39;, &#39;table1&#39;, &#39;column1&#39;], [&#39;id1&#39;, &#39;value3&#39;, &#39;value1&#39;, &#39;02/03/2020&#39;, &#39;table1&#39;, &#39;column1&#39;]], &#39;id2&#39;: [[&#39;id2&#39;, &#39;value4&#39;, &#39;value5&#39;, &#39;02/01/2020&#39;, &#39;table1&#39;, &#39;column1&#39;], [&#39;id2&#39;, &#39;value5&#39;, &#39;&#39;, &#39;02/03/2020,&#39; &#39;table1&#39;, &#39;column1&#39;]], &#39;id3&#39;: [[&#39;id3&#39;, &#39;value6&#39;, &#39;value7&#39;, &#39;02/01/2020&#39;, &#39;table1&#39;, &#39;column1&#39;], [&#39;id3&#39;, &#39;value8&#39;, &#39;value9&#39;, &#39;02/02/2020&#39;, &#39;table1&#39;, &#39;column1&#39;]], &#39;id4&#39;: [[&#39;id4&#39;, &#39;value10&#39;,&#39;value11&#39;, &#39;02/01/2020&#39;, &#39;table1&#39;, &#39;column1&#39;], [&#39;id4&#39;, &#39;value11&#39;,&#39;value12&#39;, &#39;02/02/2020&#39;, &#39;table1&#39;, &#39;column1&#39;], [&#39;id4&#39;, &#39;value12&#39;,&#39;value4&#39;, &#39;02/03/2020&#39;, &#39;table1&#39;, &#39;column1&#39;]] } . NOTE: The actual end result of a defaultdict. The more accurate representation of what the data looks like can be seen by expanding the result right below here. | . dict_by_user = defaultdict(&lt;class &#39;list&#39;&gt;, {&#39; id1&#39;: [OrderedDict([(&#39;RECORD&#39;, &#39;1&#39;), (&#39;ID&#39;, &#39; id1&#39;), (&#39;OLD_VALUE&#39;, &#39;value1&#39;), (&#39;FINAL_VALUE&#39;, &#39;value2&#39;), (&#39;DATE&#39;, &#39;02/01/2020&#39;), (&#39;TABLE&#39;, &#39; table1&#39;), (&#39;COLUMN&#39;, &#39; column1&#39;)]), OrderedDict([(&#39;RECORD&#39;, &#39;2&#39;), (&#39;ID&#39;, &#39; id1&#39;), (&#39;OLD_VALUE&#39;, &#39;value2&#39;), (&#39;FINAL_VALUE&#39;, &#39;value3&#39;), (&#39;DATE&#39;, &#39;02/02/2020&#39;), (&#39;TABLE&#39;, &#39; table1&#39;), (&#39;COLUMN&#39;, &#39; column1&#39;)]), OrderedDict([(&#39;RECORD&#39;, &#39;3&#39;), (&#39;ID&#39;, &#39; id1&#39;), (&#39;OLD_VALUE&#39;, &#39;value1&#39;), (&#39;FINAL_VALUE&#39;, &#39;value1&#39;), (&#39;DATE&#39;, &#39;02/03/2020&#39;), (&#39;TABLE&#39;, &#39; table1&#39;), (&#39;COLUMN&#39;, &#39; column1&#39;), (&#39;ORIGINAL_DATE&#39;, &#39;02/01/2020&#39;)])], &#39; id2&#39;: [OrderedDict([(&#39;RECORD&#39;, &#39;4&#39;), (&#39;ID&#39;, &#39; id2&#39;), (&#39;OLD_VALUE&#39;, &#39;value4&#39;), (&#39;FINAL_VALUE&#39;, &#39;value5&#39;), (&#39;DATE&#39;, &#39;02/01/2020&#39;), (&#39;TABLE&#39;, &#39; table1&#39;), (&#39;COLUMN&#39;, &#39; column1&#39;)]), OrderedDict([(&#39;RECORD&#39;, &#39;5&#39;), (&#39;ID&#39;, &#39; id2&#39;), (&#39;OLD_VALUE&#39;, &#39;value4&#39;), (&#39;FINAL_VALUE&#39;, &#39;&#39;), (&#39;DATE&#39;, &#39;02/03/2020&#39;), (&#39;TABLE&#39;, &#39; table1&#39;), (&#39;COLUMN&#39;, &#39; column1&#39;), (&#39;ORIGINAL_DATE&#39;, &#39;02/01/2020&#39;)])], &#39; id3&#39;: [OrderedDict([(&#39;RECORD&#39;, &#39;6&#39;), (&#39;ID&#39;, &#39; id3&#39;), (&#39;OLD_VALUE&#39;, &#39;value6&#39;), (&#39;FINAL_VALUE&#39;, &#39;value7&#39;), (&#39;DATE&#39;, &#39;02/01/2020&#39;), (&#39;TABLE&#39;, &#39; table1&#39;), (&#39;COLUMN&#39;, &#39; column1&#39;)]), OrderedDict([(&#39;RECORD&#39;, &#39;7&#39;), (&#39;ID&#39;, &#39; id3&#39;), (&#39;OLD_VALUE&#39;, &#39;value6&#39;), (&#39;FINAL_VALUE&#39;, &#39;value9&#39;), (&#39;DATE&#39;, &#39;02/02/2020&#39;), (&#39;TABLE&#39;, &#39; table1&#39;), (&#39;COLUMN&#39;, &#39; column1&#39;), (&#39;ORIGINAL_DATE&#39;, &#39;02/01/2020&#39;)])], &#39; id4&#39;: [OrderedDict([(&#39;RECORD&#39;, &#39;8&#39;), (&#39;ID&#39;, &#39; id4&#39;), (&#39;OLD_VALUE&#39;, &#39;value10&#39;), (&#39;FINAL_VALUE&#39;, &#39;value11&#39;), (&#39;DATE&#39;, &#39;02/01/2020&#39;), (&#39;TABLE&#39;, &#39; table1&#39;), (&#39;COLUMN&#39;, &#39; column1&#39;)]), OrderedDict([(&#39;RECORD&#39;, &#39;9&#39;), (&#39;ID&#39;, &#39; id4&#39;), (&#39;OLD_VALUE&#39;, &#39;value10&#39;), (&#39;FINAL_VALUE&#39;, &#39;value12&#39;), (&#39;DATE&#39;, &#39;02/02/2020&#39;), (&#39;TABLE&#39;, &#39; table1&#39;), (&#39;COLUMN&#39;, &#39; column1&#39;), (&#39;ORIGINAL_DATE&#39;, &#39;02/01/2020&#39;)])], &#39;id4&#39;: [OrderedDict([(&#39;RECORD&#39;, &#39;10&#39;), (&#39;ID&#39;, &#39;id4&#39;), (&#39;OLD_VALUE&#39;, &#39;value12&#39;), (&#39;FINAL_VALUE&#39;, &#39;value4&#39;), (&#39;DATE&#39;, &#39;02/03/2020&#39;), (&#39;TABLE&#39;, &#39; table1&#39;), (&#39;COLUMN&#39;, &#39; column1&#39;), (&#39;ORIGINAL_DATE&#39;, &#39;02/03/2020&#39;)])]}) . . dict_by_user is then passed into the next section: | . audit_summary_l = [] for i in dict_by_user: if dict_by_user: temp_dict = {} max = &#39;0&#39; min = &#39;99/99/9999&#39; for record in dict_by_user[i]: if record[&#39;DATE&#39;] &gt; max: max = record[&#39;DATE&#39;] last = record if record[&#39;DATE&#39;] &lt; min: min = record[&#39;DATE&#39;] first = record temp_dict = last temp_dict[&#39;OLD_VALUE&#39;] = first[&#39;OLD_VALUE&#39;] temp_dict[&#39;ORIGINAL_DATE&#39;] = first[&#39;DATE&#39;] audit_summary_l.append(temp_dict) columns = [&#39;ID&#39;,&#39;OLD_VALUE&#39;,&#39;ORIGINAL_DATE&#39;,&#39;FINAL_VALUE&#39;, &#39;DATE&#39;,&#39;TABLE&#39;,&#39;COLUMN&#39;,&#39;RECORD&#39;] with open(&#39;2020-06-08/audit_summary.csv&#39;, &#39;w&#39;) as output_file: dict_writer = csv.DictWriter(output_file, fieldnames=columns, lineterminator=&#39; n&#39;) dict_writer.writeheader() for data in audit_summary_l: dict_writer.writerow(data) . This results in the following csv data: | . csv ID,OLD_VALUE,ORIGINAL_DATE,FINAL_VALUE,DATE,TABLE,COLUMN,RECORD id1, value1, 02/01/2020, value1, 02/03/2020, table1,column1, 3 id2, value4, 02/01/2020, , 02/03/2020, table1,column1, 5 id3, value6, 02/01/2020, value9, 02/02/2020, table1,column1, 7 id4, value10, 02/01/2020, value4, 02/03/2020,table 1,column1, 10 .",
            "url": "https://darrida.com/csv/defaultdict/2020/06/08/Transforming-and-Flattening-Data.html",
            "relUrl": "/csv/defaultdict/2020/06/08/Transforming-and-Flattening-Data.html",
            "date": " • Jun 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://darrida.com/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://darrida.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://darrida.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}